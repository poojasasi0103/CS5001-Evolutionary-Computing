{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753737f8",
   "metadata": {},
   "source": [
    "# relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb8aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - ReLU):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.2746529 ]\n",
      " [-3.24213524]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with ReLU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    # Output layer without softmax activation\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - ReLU):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dc136",
   "metadata": {},
   "source": [
    "# tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8048b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - tanh):\n",
      "[[0.66568973]\n",
      " [0.9788486 ]\n",
      " [0.9710057 ]\n",
      " [0.70055078]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-3.78055591]\n",
      " [-2.23771772]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with tanh activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with tanh activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - tanh):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e36876",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f8d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - sigmoid):\n",
      "[[0.69060784]\n",
      " [0.90630086]\n",
      " [0.89183275]\n",
      " [0.70440877]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-3.50132496]\n",
      " [-2.2193112 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with sigmoid activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - sigmoid):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1e758",
   "metadata": {},
   "source": [
    "# elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4daecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - ELU):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.2746529 ]\n",
      " [-3.24213524]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with ELU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = elu(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with ELU activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - ELU):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618d1a8",
   "metadata": {},
   "source": [
    "# prelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a729bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[ 1.26243854]\n",
      " [-1.69793816]\n",
      " [-1.73697755]\n",
      " [-1.81055193]]\n",
      "\n",
      "A1 (Hidden layer activation - PReLU):\n",
      "[[ 1.26243854]\n",
      " [-0.77437863]\n",
      " [-0.79218332]\n",
      " [-0.82573839]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[3.61781938]\n",
      " [0.36353146]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prelu(x, alpha):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2, alpha):\n",
    "    # Hidden layer with PReLU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = prelu(Z1, alpha)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with PReLU activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Learnable parameter for PReLU\n",
    "alpha = np.random.rand()\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2, alpha)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - PReLU):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33330766",
   "metadata": {},
   "source": [
    "# leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629babbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Leaky ReLU):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.2746529 ]\n",
      " [-3.24213524]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Leaky ReLU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = leaky_relu(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Leaky ReLU activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Leaky ReLU):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eaf844",
   "metadata": {},
   "source": [
    "# selu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db76d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - SELU):\n",
      "[[0.84367273]\n",
      " [2.38433488]\n",
      " [2.2165567 ]\n",
      " [0.91240823]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.6941778 ]\n",
      " [-3.40651149]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x, alpha=1.67326, scale=1.0507):\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with SELU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = selu(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with SELU activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - SELU):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602d33f",
   "metadata": {},
   "source": [
    "# Softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ec51184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Softsign):\n",
      "[[0.4453573 ]\n",
      " [0.69412247]\n",
      " [0.67841523]\n",
      " [0.46477734]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-2.65184017]\n",
      " [-1.50536973]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softsign(x):\n",
    "    return x / (1 + np.abs(x))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Softsign activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = softsign(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Softsign activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Softsign):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddce0f",
   "metadata": {},
   "source": [
    "# softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e02f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Softplus):\n",
      "[[1.17314568]\n",
      " [2.36766623]\n",
      " [2.22407665]\n",
      " [1.21877775]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.76780502]\n",
      " [-4.18496513]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Softplus activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = softplus(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Softplus activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Softplus):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10824ba2",
   "metadata": {},
   "source": [
    "# hard sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f962ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Hard Sigmoid):\n",
      "[[0.66059251]\n",
      " [0.95385645]\n",
      " [0.92192   ]\n",
      " [0.67367626]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-3.63419225]\n",
      " [-2.15788283]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return np.maximum(0, np.minimum(1, 0.2 * x + 0.5))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Hard Sigmoid activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = hard_sigmoid(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Hard Sigmoid activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Hard Sigmoid):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb27085",
   "metadata": {},
   "source": [
    "# swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afe9cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Swish):\n",
      "[[0.55453222]\n",
      " [2.05665246]\n",
      " [1.88141036]\n",
      " [0.6116954 ]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-7.39000362]\n",
      " [-2.48761552]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def swish(x, beta=1.0):\n",
    "    return x * (1 / (1 + np.exp(-beta * x)))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Swish activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = swish(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Swish activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Swish):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e776e",
   "metadata": {},
   "source": [
    "# mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ace30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - Mish):\n",
      "[[0.6626673 ]\n",
      " [2.2297826 ]\n",
      " [2.0608056 ]\n",
      " [0.72882658]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.07038892]\n",
      " [-2.8710524 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mish(x):\n",
    "    return x * np.tanh(np.log(1 + np.exp(x)))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with Mish activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = mish(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with Mish activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - Mish):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7fa09",
   "metadata": {},
   "source": [
    "# Sigrelu= sigmoid + relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f27b4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 (Hidden layer pre-activation):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "A1 (Hidden layer activation - sigrelu):\n",
      "[[0.80296253]\n",
      " [2.26928227]\n",
      " [2.10959998]\n",
      " [0.8683813 ]]\n",
      "\n",
      "Z2 (Output layer pre-activation):\n",
      "[[-8.2746529 ]\n",
      " [-3.24213524]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigrelu(x):\n",
    "    return np.where(x > 0, np.maximum(0, x), 1 / (1 + np.exp(-x)))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer with sigrelu activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigrelu(Z1)\n",
    "    \n",
    "    # Output layer without activation (for simplicity)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    return Z1, A1, Z2\n",
    "\n",
    "# Example usage with sigrelu activation:\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Random initialization of weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(hidden_size, input_size)\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Dummy input data\n",
    "X = np.random.randn(input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1, A1, Z2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Z1 (Hidden layer pre-activation):\")\n",
    "print(Z1)\n",
    "print(\"\\nA1 (Hidden layer activation - sigrelu):\")\n",
    "print(A1)\n",
    "print(\"\\nZ2 (Output layer pre-activation):\")\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
